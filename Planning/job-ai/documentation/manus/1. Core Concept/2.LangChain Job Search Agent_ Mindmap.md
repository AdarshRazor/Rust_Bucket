# LangChain Job Search Agent: Mindmap

##   **LangChain Job Search Agent**
    
**Goal:** Automate Job Search Tasks (Gathering, Summarizing, Drafting)
**Core Technology:** LangChain (Python Framework), LLMs
**User Profile:** Beginner, seeking free/open-source solutions

## **Phase 0: Setup & Foundation**

*   Environment: Python, `venv`
*   Dependencies: `langchain`, `langchain-openai`, `pypdf`, `faiss-cpu`, `python-dotenv`, etc.
*   Configuration: API Keys (`.env`), LangSmith (Optional)
*   Guides: Introduction, Environment Setup, Framework, Architecture

## **Phase 1: Data Gathering**

**Inputs:**
*   User CV (PDF/DOCX)
*   Job Sources (Manual Collection Recommended: Text files, CSV, JSON)

**Process:**
*   Load CV: `PyPDFLoader` / `UnstructuredWordDocumentLoader`
*   Load Job Data: `DirectoryLoader` / `TextLoader` / `WebBaseLoader` (Use cautiously)

**Outputs:**
*   List of LangChain `Document` objects (CV content)
*   List of LangChain `Document` objects (Job descriptions)
* **Key Challenge:** Avoid direct LinkedIn scraping (Terms of Service, technical blocks).
* **Guide:** `phase1_data_gathering_guide.md`


## **Phase 2: Data Processing & Summarization**
*   **Inputs:**
    *   `Document` objects from Phase 1
*   **Process:**
    *   Text Splitting (Optional but recommended for long docs): `RecursiveCharacterTextSplitter` -> Chunks
    *   Information Extraction:
        *   Define Structure: `Pydantic` Model (`JobDetails`)
        *   Setup: LLM (`ChatOpenAI`), Parser (`PydanticOutputParser`)
        *   Chain: Prompt Template + LLM + Parser -> Structured `JobDetails` object
    *   Summarization:
        *   Setup: LLM (`ChatOpenAI`)
        *   Chain: `load_summarize_chain` (e.g., `map_reduce`) -> Job Summary String
*   **Outputs:**
    *   Structured Job Information (Python objects/dictionaries)
    *   Concise Job Summaries (strings)
    *   (Store these outputs, e.g., in memory or JSON)
*   **Guide:** `phase2_data_processing_guide.md`

## **Phase 3: Communication Preparation (RAG)**
*   **Inputs:**
    *   CV Chunks (from Phase 2 splitting or re-split)
    *   Structured `JobDetails` (from Phase 2)
    *   Job Summaries (from Phase 2)
*   **Process:**
    *   Setup RAG:
        *   Embeddings: `OpenAIEmbeddings`
        *   Vector Store: `FAISS` (`from_documents`) -> `cv_vectorstore`
        *   Retriever: `cv_vectorstore.as_retriever()`
    *   Draft Email:
        *   Query Retriever: Job Details -> Relevant CV Chunks
        *   Chain: Prompt (Job Details + CV Context) + LLM + Parser -> Email Draft
    *   Draft DM Hook:
        *   Query Retriever: Job Details -> Relevant CV Chunks
        *   Chain: Prompt (Job Details + CV Context + Target Info) + LLM + Parser -> DM Draft
*   **Outputs:**
    *   Personalized Email Drafts (strings)
    *   Personalized DM Hooks (strings)
*   **Key Concept:** Retrieval-Augmented Generation (RAG) for personalization.
*   **Guide:** `phase3_communication_prep_guide.md`

## **Phase 4: Orchestration & Planning**
*   **Inputs:**
    *   Access to functions/chains from Phases 1-3
    *   Configuration (File paths)
*   **Process (Choose Method):**
    *   **Method 1 (Recommended): Simple Sequential Script**
        *   Load CV -> Setup Retriever -> Load Jobs -> Loop (Extract -> Summarize -> Draft Email -> Draft DM) -> Collect Results -> Save Output (JSON)
    *   Method 2 (Advanced): LangChain Agent
        *   Define `Tools` (wrapping Phase 1-3 functions)
        *   Setup `AgentExecutor`
        *   Run with high-level objective
*   **Outputs:**
    *   Consolidated Results File (e.g., `job_agent_results.json`)
    *   User Action Plan: Review JSON, Refine Drafts, Send Manually
*   **Guide:** `phase4_orchestration_guide.md`

## **Supporting Documents:**
*   `challenges_best_practices.md`
*   `alternatives.md`
*   `consolidated_code_snippets.md`
*   `detailed_todos.md` [Todo](../todo.md)